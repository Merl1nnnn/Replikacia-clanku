import os
import numpy as np
import cv2
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.applications import VGG16, Xception
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess
from tensorflow.keras.applications.xception import preprocess_input as xcep_preprocess
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
from xgboost import XGBClassifier
import joblib
import logging
import gc
import matplotlib.pyplot as plt
import seaborn as sns
import random

# === –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ===
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === –ù–ê–°–¢–†–û–ô–ö–ê GPU ===
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    logger.info("‚úÖ GPU –¥–æ—Å—Ç—É–ø–µ–Ω: %s", gpus)
else:
    logger.warning("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")

# === –ü–ê–†–ê–ú–ï–¢–†–´ ===
IMG_SIZE = 128
DATASET_PATH = "grape_leaf_dataset"
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
# === –§–£–ù–ö–¶–ò–ò ===

def remove_background(image):
    img_np = np.array(image)
    img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
    hsv = cv2.cvtColor(img_cv, cv2.COLOR_BGR2HSV)
    lower_green = np.array([25, 40, 40])
    upper_green = np.array([85, 255, 255])
    mask = cv2.inRange(hsv, lower_green, upper_green)
    result = cv2.bitwise_and(img_cv, img_cv, mask=mask)
    result[mask == 0] = [255, 255, 255]
    return result

def preprocess_image(path):
    try:
        image = Image.open(path).convert("RGB")
        img = remove_background(image)

        # üîπ –ü—Ä–∏—ë–º–∏—Ä–æ–≤–∞–Ω–∏–µ: –ø—Ä–æ—Å—Ç–æ —Å–≥–ª–∞–∂–∏–≤–∞–µ–º —è—Ä–∫–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Ä–∞–∑–º—ã—Ç–∏–µ
        img = cv2.blur(img, (3, 3))  # —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ 3x3 –æ–∫–Ω—É

        # üî∏ –ë–∏–ª–∞—Ç–µ—Ä–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –º—è–≥–∫–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≥—Ä–∞–Ω–∏—Ü
        img = cv2.bilateralFilter(img, d=9, sigmaColor=75, sigmaSpace=75)

        resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
        return resized / 255.0
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –≤ %s: %s", path, e)
        return np.zeros((IMG_SIZE, IMG_SIZE, 3))
    
def augment_image(image, class_idx):
    if class_idx in [0, 1]:  # –î–ª—è –∫–ª–∞—Å—Å–æ–≤ 0 –∏ 1 –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–≥–ª–∞–∂–µ–Ω–Ω—É—é –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é
        datagen = ImageDataGenerator(
            rotation_range=10,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –≤—Ä–∞—â–µ–Ω–∏—è
            width_shift_range=0.05,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ —Å–¥–≤–∏–≥–∞
            height_shift_range=0.05,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–≥–æ —Å–¥–≤–∏–≥–∞
            zoom_range=0.05,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
            horizontal_flip=True  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ
        )
    else:  # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        datagen = ImageDataGenerator(
            rotation_range=25,
            width_shift_range=0.1,
            height_shift_range=0.1,
            zoom_range=0.1,
            horizontal_flip=True
        )
    
    image = np.expand_dims(image, 0)
    return [datagen.flow(image, batch_size=1).__next__()[0]]

def load_dataset(path, max_per_class=500, max_per_difficult_class=1000, test_ratio=0.15, 
                 train_file='train.txt', val_file='val.txt', test_file='test.txt',
                 augment_count=1):  # –°–∫–æ–ª—å–∫–æ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–ø–∏–π –¥–µ–ª–∞—Ç—å
    X_train, y_train, X_val, y_val, X_test, y_test = [], [], [], [], [], []
    class_map = {name: idx for idx, name in enumerate(sorted(os.listdir(path)))}
    logger.info("–ö–ª–∞—Å—Å—ã: %s", class_map)

    train_paths, val_paths, test_paths = [], [], []

    for label, idx in class_map.items():
        folder = os.path.join(path, label)
        images = os.listdir(folder)
        np.random.shuffle(images)

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ —á–∏—Å–ª—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if idx in [0, 1]:
            images = images[:max_per_difficult_class]
        elif idx in [2, 3]:
            images = images[:max_per_class]
        else:
            logger.warning(f"–ö–ª–∞—Å—Å {idx} –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
            continue

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train / val / test
        num_test = int(len(images) * test_ratio)
        num_val = int(len(images) * 0.15)
        test_images = images[:num_test]
        val_images = images[num_test:num_test + num_val]
        train_images = images[num_test + num_val:]

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        def process_and_append(img_list, X, y, paths, label_idx, augment=False):
            for img_name in img_list:
                img_path = os.path.join(folder, img_name)
                base_img = preprocess_image(img_path)
                X.append(base_img)
                y.append(label_idx)
                paths.append(img_path)

                # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è train
                if augment:
                    aug_images = augment_image(base_img, label_idx)
                    for aug in aug_images[:augment_count]:
                        X.append(aug)
                        y.append(label_idx)

        process_and_append(train_images, X_train, y_train, train_paths, idx, augment=True)
        process_and_append(val_images, X_val, y_val, val_paths, idx, augment=False)
        process_and_append(test_images, X_test, y_test, test_paths, idx, augment=False)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç–∏
    def save_paths(paths, filename):
        with open(filename, 'w') as f:
            for path in paths:
                f.write(path + '\n')

    save_paths(train_paths, train_file)
    save_paths(val_paths, val_file)
    save_paths(test_paths, test_file)

    return (
        np.array(X_train), np.array(y_train),
        np.array(X_val), np.array(y_val),
        np.array(X_test), np.array(y_test),
        class_map
    )

def extract_features(model, X, preprocess_fn, batch_size=32):
    features = []
    for i in range(0, len(X), batch_size):
        batch = X[i:i+batch_size]
        batch_prep = preprocess_fn(batch * 255.0)

        # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ GPU
        with tf.device('/GPU:0'):
            feats = model.predict(batch_prep, verbose=0)
        
        features.extend([f.flatten() for f in feats])
        gc.collect()
    return np.array(features)

def plot_confusion_matrix(cm, class_names):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', cbar=False,
                xticklabels=class_names, yticklabels=class_names, linewidths=0.5, linecolor='gray')
    plt.title("Confusion Matrix", fontsize=14)
    plt.xlabel("Predicted Class", fontsize=12)
    plt.ylabel("True Class", fontsize=12)
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

def main():
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    X_train, y_train, X_val, y_val, X_test, y_test, class_map = load_dataset(DATASET_PATH)

    gpus = tf.config.list_physical_devices('GPU')

    if gpus:
        print(f"‚úÖ GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {gpus}")
    else:
        print("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")
    tf.config.set_visible_devices(gpus[0], 'GPU')

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º class_map –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    logger.info("–°–æ—Ö—Ä–∞–Ω—è–µ–º class_map –≤ —Ñ–∞–π–ª...")
    np.save('class_map.npy', class_map)

    vgg = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
    xcep = Xception(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
    with tf.device('/GPU:0'):  # /GPU:0 ‚Äî –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π GPU
        vgg_model = Model(inputs=vgg.input, outputs=vgg.output)
        xcep_model = Model(inputs=xcep.input, outputs=xcep.output)

    logger.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ VGG16...")
    vgg_train = extract_features(vgg_model, X_train, vgg_preprocess)
    vgg_val = extract_features(vgg_model, X_val, vgg_preprocess)
    vgg_test = extract_features(vgg_model, X_test, vgg_preprocess)

    # –£–¥–∞–ª—è–µ–º –º–æ–¥–µ–ª—å VGG16 –∏–∑ –ø–∞–º—è—Ç–∏
    del vgg, vgg_model
    gc.collect()

    logger.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ Xception...")
    xcep_train = extract_features(xcep_model, X_train, xcep_preprocess)
    xcep_val = extract_features(xcep_model, X_val, xcep_preprocess)
    xcep_test = extract_features(xcep_model, X_test, xcep_preprocess)

    # –£–¥–∞–ª—è–µ–º –º–æ–¥–µ–ª—å Xception –∏–∑ –ø–∞–º—è—Ç–∏
    del xcep, xcep_model
    gc.collect()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    X_train_comb = np.concatenate([vgg_train, xcep_train], axis=1)
    X_val_comb = np.concatenate([vgg_val, xcep_val], axis=1)
    X_test_comb = np.concatenate([vgg_test, xcep_test], axis=1)

    # –£–¥–∞–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    del vgg_train, vgg_val, vgg_test, xcep_train, xcep_val, xcep_test
    gc.collect()

    # –û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏
    logger.info("–û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏...")
    clf = XGBClassifier(
        n_estimators=100,
        max_depth=4,
        learning_rate=0.2,
        random_state=SEED,
        tree_method='hist',
        device='cuda',
        n_jobs=-1
    )
    clf.fit(X_train_comb, y_train, eval_set=[(X_val_comb, y_val)], verbose=True)

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    logger.info("–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
    y_pred = clf.predict(X_test_comb)
    cm = confusion_matrix(y_test, y_pred)
    logger.info("Confusion Matrix:")
    logger.info(cm)

    # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞ –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    report = classification_report(y_test, y_pred, target_names=class_map.keys())
    logger.info("\nClassification Report:")
    logger.info(report)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    joblib.dump(clf, "xgb_model.pkl")
    logger.info("‚úÖ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ xgb_model.pkl")
    
    # –£–¥–∞–ª—è–µ–º –º–æ–¥–µ–ª—å XGBoost –∏–∑ –ø–∞–º—è—Ç–∏
    del clf
    
    gc.collect()

if __name__ == "__main__":
    main()
